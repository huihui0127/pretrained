pretrained_path = "E:/aNewD\project\mathbert\pretrained\data/256.txt"
pretrained_eval_path = "E:/aNewD\project\mathbert\pretrained\data/256.txt"
max_length = 256
do_train = True
do_eval = False
#do_lower = True    #有英文
train_batch_size = 32
eval_batch_size = 32
lr = 1e-4
warmup_proportion = 0.1
train_epoch = 100
save_checkpoint_steps = 15000 #梯度下降15000次
# self.pretrain_model_path = '/home/pretrained_model/nezha_base/' #二次预训练需要的
dupe_factor = 1#防止过拟合
local_rank = -1 #是否分布式训练
seed = 42
no_cuda = False
gradient_accumulation_steps = 1 #显存够的时候 积累
fp16 = False
bert_config_json = "bert_config.json"
vocab_file = "E:/aNewD\project\mathbert\pretrained\data/vocab.txt" #标橙色的斜杠反过来
loss_scale = 0
init_model = ""
mask_proportion = 0.15
mask_prediction_max_sequence = 20
output_dir = "./data/saved/"
frozen = True

